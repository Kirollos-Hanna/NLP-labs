{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c25f40b-a014-4434-be34-29eb7662e9e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1:\n",
    "examples from chapter 2 section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d37127-8714-4325-a40b-c2d9e002b668",
   "metadata": {},
   "source": [
    "### Senses and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa02f1c-10e9-4f62-9242-9f0c11ebd607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd0322f-9e06-4147-81b4-ece91f138753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "672e232b-3c1d-4a75-837c-7aaf8b786e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf63e84-739e-4eac-98f1-88e13d1293d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ee45105-be37-4692-aab3-e9b109997909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99205cb-61ac-444d-aacd-3eef479854d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('car.n.01.automobile')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9209fd21-fb62-4b9d-bc3a-90897310990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53e59ecd-3ebe-4cc5-83a0-f42b415a6bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').name() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38210577-515b-4100-8f00-7d6c20ad1f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d761c9-b56a-4c96-85fb-433f82aaecf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('car'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9ec9620-b088-47b4-8fc2-fb7cbe55fe79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.02.car'),\n",
       " Lemma('car.n.03.car'),\n",
       " Lemma('car.n.04.car'),\n",
       " Lemma('cable_car.n.01.car')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('car')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea21bb-f412-403e-b65b-4a25eb0ba6ab",
   "metadata": {},
   "source": [
    "#### All senses of the word \"dish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2dcabb3-40e8-44eb-9034-b8dc654fa648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dish_synsets = wn.synsets('dish')\n",
    "dish_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75976bf8-f720-46e9-8509-b12e57e464e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition:  a piece of dishware normally used as a container for holding or serving food\n",
      "Examples:  ['we gave them a set of dishes for a wedding present']\n",
      "Lemma names:  ['dish']\n",
      "\n",
      "\n",
      "Definition:  a particular item of prepared food\n",
      "Examples:  ['she prepared a special dish for dinner']\n",
      "Lemma names:  ['dish']\n",
      "\n",
      "\n",
      "Definition:  the quantity that a dish will hold\n",
      "Examples:  ['they served me a dish of rice']\n",
      "Lemma names:  ['dish', 'dishful']\n",
      "\n",
      "\n",
      "Definition:  a very attractive or seductive looking woman\n",
      "Examples:  []\n",
      "Lemma names:  ['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
      "\n",
      "\n",
      "Definition:  directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
      "Examples:  []\n",
      "Lemma names:  ['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
      "\n",
      "\n",
      "Definition:  an activity that you like or at which you are superior\n",
      "Examples:  ['chemistry is not my cup of tea', 'his bag now is learning to play golf', 'marriage was scarcely his dish']\n",
      "Lemma names:  ['cup_of_tea', 'bag', 'dish']\n",
      "\n",
      "\n",
      "Definition:  provide (usually but not necessarily food)\n",
      "Examples:  ['We serve meals for the homeless', 'She dished out the soup at 8 P.M.', 'The entertainers served up a lively show']\n",
      "Lemma names:  ['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
      "\n",
      "\n",
      "Definition:  make concave; shape like a dish\n",
      "Examples:  []\n",
      "Lemma names:  ['dish']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in dish_synsets:\n",
    "    print(\"Definition: \", synset.definition())\n",
    "    print(\"Examples: \", synset.examples())\n",
    "    print(\"Lemma names: \", synset.lemma_names())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a4e197-2311-4ed8-a31e-f7a1922450b8",
   "metadata": {},
   "source": [
    "### WordNet Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf634519-6037-4f1e-83e0-12768488463a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('ambulance.n.01')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bb33d49-f59d-43e7-8d66-ae14f2bad2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Model_T',\n",
       " 'S.U.V.',\n",
       " 'SUV',\n",
       " 'Stanley_Steamer',\n",
       " 'ambulance',\n",
       " 'beach_waggon',\n",
       " 'beach_wagon',\n",
       " 'bus',\n",
       " 'cab',\n",
       " 'compact',\n",
       " 'compact_car',\n",
       " 'convertible',\n",
       " 'coupe',\n",
       " 'cruiser',\n",
       " 'electric',\n",
       " 'electric_automobile',\n",
       " 'electric_car',\n",
       " 'estate_car',\n",
       " 'gas_guzzler',\n",
       " 'hack',\n",
       " 'hardtop',\n",
       " 'hatchback',\n",
       " 'heap',\n",
       " 'horseless_carriage',\n",
       " 'hot-rod',\n",
       " 'hot_rod',\n",
       " 'jalopy',\n",
       " 'jeep',\n",
       " 'landrover',\n",
       " 'limo',\n",
       " 'limousine',\n",
       " 'loaner',\n",
       " 'minicar',\n",
       " 'minivan',\n",
       " 'pace_car',\n",
       " 'patrol_car',\n",
       " 'phaeton',\n",
       " 'police_car',\n",
       " 'police_cruiser',\n",
       " 'prowl_car',\n",
       " 'race_car',\n",
       " 'racer',\n",
       " 'racing_car',\n",
       " 'roadster',\n",
       " 'runabout',\n",
       " 'saloon',\n",
       " 'secondhand_car',\n",
       " 'sedan',\n",
       " 'sport_car',\n",
       " 'sport_utility',\n",
       " 'sport_utility_vehicle',\n",
       " 'sports_car',\n",
       " 'squad_car',\n",
       " 'station_waggon',\n",
       " 'station_wagon',\n",
       " 'stock_car',\n",
       " 'subcompact',\n",
       " 'subcompact_car',\n",
       " 'taxi',\n",
       " 'taxicab',\n",
       " 'tourer',\n",
       " 'touring_car',\n",
       " 'two-seater',\n",
       " 'used-car',\n",
       " 'waggon',\n",
       " 'wagon']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e607957-80c2-44d9-a37b-e3f27b6debae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea4f26bd-aa4b-4524-9cb5-655d938a71c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = motorcar.hypernym_paths()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a2265b7-d4eb-4e0d-8444-ee85fe44aea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'container.n.01',\n",
       " 'wheeled_vehicle.n.01',\n",
       " 'self-propelled_vehicle.n.01',\n",
       " 'motor_vehicle.n.01',\n",
       " 'car.n.01']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d4fae6-7548-4375-83f0-2afff319591f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'conveyance.n.03',\n",
       " 'vehicle.n.01',\n",
       " 'wheeled_vehicle.n.01',\n",
       " 'self-propelled_vehicle.n.01',\n",
       " 'motor_vehicle.n.01',\n",
       " 'car.n.01']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92dc0ffc-6979-4158-9567-34b241566de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar.root_hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275c836-aca4-4963-a091-c8ee248ac832",
   "metadata": {},
   "source": [
    "### Lexical Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96e721f3-881b-4b9d-9445-3c35b180bd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b18db31b-a1a6-4920-9a4d-4804427e3ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eba1379-718d-41f6-a3fb-4fa3631c8301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e80d151-23b1-4cbd-a5e7-d22da9a7fa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.n.02: (often followed by `of') a large number or amount or extent\n",
      "mint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and small mauve flowers\n",
      "mint.n.03: any member of the mint family of plants\n",
      "mint.n.04: the leaves of a mint plant used fresh or candied\n",
      "mint.n.05: a candy that is flavored with a mint oil\n",
      "mint.n.06: a plant where money is coined by authority of the government\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "     print(synset.name() + ':', synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170be852-b9f0-47cd-b9d4-a855a4661ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.02')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c30bef11-1126-4ea1-a09c-e1dacdb0639c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.05')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').substance_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135e985-dd6c-46da-8bf0-25a5f20b85a8",
   "metadata": {},
   "source": [
    "### Entailments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecbd6876-2793-4cc7-a924-85ec9d647a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09cac054-b17a-4aca-bd00-1f73250bb3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chew.v.01'), Synset('swallow.v.01')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ae2beb7-d54c-4e1c-b991-01ad1d9bb59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('arouse.v.07'), Synset('disappoint.v.01')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tease.v.03').entailments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addd2c0-968e-46cf-b16a-8bb7ace51abe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Antonymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab34a047-b444-4458-b43a-3ffe187dffb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "816a1ace-143d-4a6d-985a-5d30cf98950e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('linger.v.04.linger')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb834b0a-cfaa-483f-ab80-a64968b0cae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('vertical.a.01.vertical'), Lemma('inclined.a.02.inclined')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c1e5e0f-fdda-4384-bbaa-560bb733da18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('legato.r.01.legato')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed26cb-1a57-4d02-b814-e20cf2a1fec4",
   "metadata": {},
   "source": [
    "### Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38f8c2f3-fb10-4189-85f4-26c27403d719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('baleen_whale.n.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "right.lowest_common_hypernyms(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09529bc3-7d88-402f-a4ed-c4623af82117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e29bda87-cd4f-4417-9cd3-341b35b638ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertebrate.n.01')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de0212fa-5059-4ae0-af48-ce00fb537796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0baecec3-72c5-4ba5-8c6e-759c4074ce68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('baleen_whale.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d32af179-31bb-4b16-84d2-0567447c5fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('whale.n.02').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b166a5cb-7413-4020-aba0-57931e7f3a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('vertebrate.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6954d284-b6f8-4781-9cd1-eba857c4664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('entity.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e359dfa7-e348-4a61-81c9-23524a64acad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9df4154-d003-43bd-ad0e-db20aaebe2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f13f418-92cd-429b-b729-c85054f8877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98ab6eee-715b-4836-b276-d0565886dbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043478260869565216"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1255a69-8ac5-498d-a211-1559092277e0",
   "metadata": {},
   "source": [
    "### Own Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1a391-1081-4ad9-bf82-1867f5cb447f",
   "metadata": {},
   "source": [
    "#### Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ab3c42b-0621-44ba-955d-64cc8fb44f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('building.n.01'),\n",
       " Synset('construction.n.01'),\n",
       " Synset('construction.n.07'),\n",
       " Synset('building.n.04'),\n",
       " Synset('construct.v.01'),\n",
       " Synset('build_up.v.02'),\n",
       " Synset('build.v.03'),\n",
       " Synset('build.v.04'),\n",
       " Synset('build.v.05'),\n",
       " Synset('build.v.06'),\n",
       " Synset('build.v.07'),\n",
       " Synset('build.v.08'),\n",
       " Synset('build_up.v.04'),\n",
       " Synset('build.v.10')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_synsets = wn.synsets(\"building\")\n",
    "building_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f993d7ca-dbc4-4d6a-9e24-55174ec6a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition:  a structure that has a roof and walls and stands more or less permanently in one place\n",
      "Examples:  ['there was a three-story building on the corner', 'it was an imposing edifice']\n",
      "Lemma names:  ['building', 'edifice']\n",
      "\n",
      "\n",
      "Definition:  the act of constructing something\n",
      "Examples:  ['during the construction we had to take a detour', 'his hobby was the building of boats']\n",
      "Lemma names:  ['construction', 'building']\n",
      "\n",
      "\n",
      "Definition:  the commercial activity involved in repairing old structures or constructing new ones\n",
      "Examples:  ['their main business is home construction', 'workers in the building trades']\n",
      "Lemma names:  ['construction', 'building']\n",
      "\n",
      "\n",
      "Definition:  the occupants of a building\n",
      "Examples:  ['the entire building complained about the noise']\n",
      "Lemma names:  ['building']\n",
      "\n",
      "\n",
      "Definition:  make by combining materials and parts\n",
      "Examples:  ['this little pig made his house out of straw', 'Some eccentric constructed an electric brassiere warmer']\n",
      "Lemma names:  ['construct', 'build', 'make']\n",
      "\n",
      "\n",
      "Definition:  form or accumulate steadily\n",
      "Examples:  [\"Resistance to the manager's plan built up quickly\", 'Pressure is building up at the Indian-Pakistani border']\n",
      "Lemma names:  ['build_up', 'work_up', 'build', 'progress']\n",
      "\n",
      "\n",
      "Definition:  build or establish something abstract\n",
      "Examples:  ['build a reputation']\n",
      "Lemma names:  ['build', 'establish']\n",
      "\n",
      "\n",
      "Definition:  improve the cleansing action of\n",
      "Examples:  ['build detergents']\n",
      "Lemma names:  ['build']\n",
      "\n",
      "\n",
      "Definition:  order, supervise, or finance the construction of\n",
      "Examples:  ['The government is building new schools in this state']\n",
      "Lemma names:  ['build']\n",
      "\n",
      "\n",
      "Definition:  give form to, according to a plan\n",
      "Examples:  ['build a modern nation', 'build a million-dollar business']\n",
      "Lemma names:  ['build']\n",
      "\n",
      "\n",
      "Definition:  be engaged in building\n",
      "Examples:  ['These architects build in interesting and new styles']\n",
      "Lemma names:  ['build']\n",
      "\n",
      "\n",
      "Definition:  found or ground\n",
      "Examples:  [\"build a defense on nothing but the accused person's reputation\"]\n",
      "Lemma names:  ['build']\n",
      "\n",
      "\n",
      "Definition:  bolster or strengthen\n",
      "Examples:  ['We worked up courage', 'build up confidence', 'ramp up security in the airports']\n",
      "Lemma names:  ['build_up', 'work_up', 'build', 'ramp_up']\n",
      "\n",
      "\n",
      "Definition:  develop and grow\n",
      "Examples:  ['Suspense was building right from the beginning of the opera']\n",
      "Lemma names:  ['build']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for synset in building_synsets:\n",
    "    print(\"Definition: \", synset.definition())\n",
    "    print(\"Examples: \", synset.examples())\n",
    "    print(\"Lemma names: \", synset.lemma_names())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe5d7e8-f0d4-493f-a7b4-0d9a00fea4c9",
   "metadata": {},
   "source": [
    "#### hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b460b4dc-4da8-4b47-832e-a996e7c93fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hall_of_Fame', 'Roman_building', 'abattoir', 'apartment_building', 'apartment_house', 'architecture', 'aviary', 'bagnio', 'bathhouse', 'bathing_machine', 'bawdyhouse', 'bird_sanctuary', 'bordello', 'bowling_alley', 'brothel', 'butchery', 'casino-hotel', 'cathouse', 'center', 'centre', 'chapterhouse', 'club', 'clubhouse', 'dead_room', 'dorm', 'dormitory', 'eatery', 'eating_house', 'eating_place', 'farm_building', 'feedlot', 'firetrap', 'gambling_den', 'gambling_hell', 'gambling_house', 'gaming_house', 'gazebo', 'glasshouse', 'government_building', 'greenhouse', 'hall', 'hall', 'hall', 'health_facility', 'healthcare_facility', 'hotel', 'hotel-casino', 'house', 'house', 'house', 'house_of_God', 'house_of_ill_repute', 'house_of_prayer', 'house_of_prostitution', 'house_of_worship', 'library', 'medical_building', 'ministry', 'morgue', 'mortuary', 'nursery', 'observatory', 'office_block', 'office_building', 'opium_den', 'outbuilding', 'packinghouse', 'place_of_worship', 'planetarium', 'presbytery', 'residence_hall', 'rest_house', 'restaurant', 'rink', 'rotunda', 'ruin', 'school', 'schoolhouse', 'shambles', 'shooting_gallery', 'signal_box', 'signal_tower', 'skating_rink', 'skyscraper', 'slaughterhouse', 'sporting_house', 'student_residence', 'student_union', 'summerhouse', 'tap_house', 'tavern', 'telco_building', 'telecom_hotel', 'temple', 'theater', 'theatre', 'volary', 'whorehouse']\n",
      "['assembly', 'crenelation', 'crenellation', 'dry_walling', 'erecting', 'erection', 'fabrication', 'grading', 'house-raising', 'leveling', 'road_construction', 'rustication', 'ship_building', 'shipbuilding']\n",
      "['jerry-building']\n",
      "['cantilever', 'channelise', 'channelize', 'corduroy', 'customise', 'customize', 'dry-wall', 'erect', 'frame', 'frame_up', 'groin', 'lock', 'put_up', 'raise', 'rear', 'rebuild', 'reconstruct', 'revet', 'set_up', 'wattle']\n"
     ]
    }
   ],
   "source": [
    "for synset in building_synsets:\n",
    "    types_of_building = synset.hyponyms()\n",
    "    if len(types_of_building) > 0:\n",
    "        print(sorted(lemma.name() for synset in types_of_building for lemma in synset.lemmas()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e307b-8139-4ff8-8a14-e5d03a27e743",
   "metadata": {},
   "source": [
    "#### hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f81835d5-0daa-4def-995e-600f701060f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('structure.n.01')\n",
      "Synset('creating_from_raw_materials.n.01')\n",
      "Synset('commercial_enterprise.n.02')\n",
      "Synset('gathering.n.01')\n",
      "Synset('make.v.03')\n",
      "Synset('develop.v.10')\n",
      "Synset('make.v.03')\n",
      "Synset('better.v.02')\n",
      "Synset('oversee.v.01')\n",
      "Synset('develop.v.01')\n",
      "Synset('create.v.03')\n",
      "Synset('establish.v.08')\n",
      "Synset('increase.v.02')\n",
      "Synset('intensify.v.03')\n"
     ]
    }
   ],
   "source": [
    "for synset in building_synsets:\n",
    "    types_of_building = synset.hypernyms()\n",
    "    if len(types_of_building) > 0:\n",
    "        print(types_of_building[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbbdd92-a2bb-42fc-93f9-6b1022cc2e04",
   "metadata": {},
   "source": [
    "#### semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "445df31c-40d0-4a3e-b630-830308e34e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chief_executive_officer.n.01')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo = wn.synsets(\"ceo\")\n",
    "ceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1432a0b-8e24-4030-9bd3-a4b5b430fdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('chief_executive_officer.n.01')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo_sysnet = wn.synset(\"chief_executive_officer.n.01\")\n",
    "ceo_sysnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3546642-2236-4de1-ac80-2e74e1c94dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whole.n.02')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_synsets[0].lowest_common_hypernyms(ceo_sysnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "aa8c52a8-ba53-4cb0-8654-8078065242ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('whole.n.02').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ae0a5297-87b9-4a1e-a400-70bb60105ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_synsets[0].path_similarity(ceo_sysnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a25d65-5b1d-44e0-a585-03fa62efdbfb",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Identify the synsets of the word “car” and rank them in the order of their frequency of occurrence\n",
    "\n",
    "(most common synset first, less common synset at the end). For this purpose, you may use the coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f2ae5248-2813-41af-b00c-b84429948723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "car = wn.synsets('car', 'n')[0] # Get the most common synset\n",
    "\n",
    "print(car.lemmas()[0].count()) # Get the first lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c800c4f8-e3a0-4c69-964e-8a7a31c7ff58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_synsets = wn.synsets('car', 'n')\n",
    "car_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "907f85bc-ebda-4518-bd9b-c96a41a46c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Synset('car.n.01'): 89,\n",
       " Synset('car.n.02'): 2,\n",
       " Synset('car.n.03'): 0,\n",
       " Synset('car.n.04'): 0,\n",
       " Synset('cable_car.n.01'): 0}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_synset_frequencies = dict.fromkeys(car_synsets,0)\n",
    "\n",
    "for car_sysnet in car_synsets:\n",
    "    for car_lemma in car_sysnet.lemmas():\n",
    "        car_synset_frequencies[car_sysnet] += car_lemma.count()\n",
    "\n",
    "car_synset_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea19c0e9-eadb-489c-9138-d2e0da3071f9",
   "metadata": {},
   "source": [
    "## Steps 3 and 4:\n",
    "Now consider two sentences T1 and T2, each constituted with a set of tokens. For this purpose, study expression (1) of the aforementioned Mihalcea et al.’s paper above (see below).  You can check with a potential implementation available at https://nlpforhackers.io/wordnet-sentence-similarity/ \n",
    "\n",
    "Start with sentences: T1: “Students feel unhappy today about the class”. T2: ”Many students struggled to understand some key concepts about the subject seen in the class”,  and study the influence of various preprocessing (stopword removal, stemming) on the result of the sentence-to-sentence similarity above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd933edd-96a2-438c-9936-8f2551ec2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = \"Students feel unhappy today about the class\"\n",
    "t2 = \"Many students struggled to understand some key concepts about the subject seen in the class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0318273f-567d-443e-a742-f77d26a9552e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bemob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "725b12fd-db87-41ca-8759-84f31c9638a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(\"Students feel unhappy today about the class\", \"Many students struggled to understand some key concepts about the subject seen in the class\") = 0.5583333333333333\n",
      "Similarity(\"Many students struggled to understand some key concepts about the subject seen in the class\", \"Students feel unhappy today about the class\") = 0.41155202821869485\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.path_similarity(ss) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "\n",
    "t1 = \"Students feel unhappy today about the class\"\n",
    "t2 = \"Many students struggled to understand some key concepts about the subject seen in the class\"\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (t1, t2, sentence_similarity(t1, t2)))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (t2, t1, sentence_similarity(t2, t1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c95c4e2-027b-4aa9-9d45-29ef88b1cb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(\"Students feel unhappy today about the class\", \"Many students struggled to understand some key concepts about the subject seen in the class\") = 0.4849426807760141\n",
      "Similarity(\"Many students struggled to understand some key concepts about the subject seen in the class\", \"Students feel unhappy today about the class\") = 0.4849426807760141\n"
     ]
    }
   ],
   "source": [
    "def symmetric_sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_similarity(sentence1, sentence2) + sentence_similarity(sentence2, sentence1)) / 2\n",
    "\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (t1, t2, symmetric_sentence_similarity(t1, t2)))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (t2, t1, symmetric_sentence_similarity(t2, t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984c8d1-744d-4014-8c15-7364503d947c",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Use a set of pair of sentences of your choice, starting with sentences that have close semantic meaning to pairs that are very disparate from each other, and report how the semantic similarity of the two sentences varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4f454f5b-cafa-4930-a326-ae9feca9c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(\"I eat icecream all the time\", \"All I eat is icecream\") = 0.5902777777777778\n",
      "Similarity(\"All I eat is icecream\", \"I eat icecream all the time\") = 0.5902777777777778\n"
     ]
    }
   ],
   "source": [
    "similar_sentences = [\"I eat icecream all the time\", \"All I eat is icecream\"]\n",
    "\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (similar_sentences[0], similar_sentences[1], symmetric_sentence_similarity(similar_sentences[0], similar_sentences[1])))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (similar_sentences[1], similar_sentences[0], symmetric_sentence_similarity(similar_sentences[1], similar_sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d3023df3-ca24-424b-99bb-4922c7f2a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(\"I love movies very much\", \"How is the weather in your country\") = 0.22777777777777775\n",
      "Similarity(\"How is the weather in your country\", \"I love movies very much\") = 0.22777777777777775\n"
     ]
    }
   ],
   "source": [
    "disparate_sentences = [\"I love movies very much\", \"How is the weather in your country\"]\n",
    "\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (disparate_sentences[0], disparate_sentences[1], symmetric_sentence_similarity(disparate_sentences[0], disparate_sentences[1])))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (disparate_sentences[1], disparate_sentences[0], symmetric_sentence_similarity(disparate_sentences[1], disparate_sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad2bf8-7b3f-4e81-8eb9-66f57ea6a0f4",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Test the various available word-to-word semantic similarity (e.g., Wu and Palmer, Resnik, path length) and how they contribute to changing overall semantic similarity of the pair of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "20a94c39-956e-4352-b15e-c0b52298ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity_wu(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.wup_similarity(ss) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "\n",
    "def symmetric_sentence_similarity_wu(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_similarity_wu(sentence1, sentence2) + sentence_similarity_wu(sentence2, sentence1)) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dea168b1-9e1b-4065-a7e0-7ad33a066306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     C:\\Users\\bemob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet_ic.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6e6df293-ba10-4bd6-aad5-4f1739ddbff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    "# Wordnet information content file\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "def sentence_similarity_resnik(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.res_similarity(ss,brown_ic) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "\n",
    "def symmetric_sentence_similarity_resnik(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_similarity_resnik(sentence1, sentence2) + sentence_similarity_resnik(sentence2, sentence1)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "59eadbe1-04fb-4ab6-8b66-513087226a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity(\"I eat icecream all the time\", \"All I eat is icecream\") = 0.5902777777777778\n",
      "Similarity(\"I love movies very much\", \"How is the weather in Ohio\") = 0.22499999999999998\n",
      "Similarity(\"I eat icecream all the time\", \"All I eat is icecream\") = 0.6499999999999999\n",
      "Similarity(\"I love movies very much\", \"How is the weather in Ohio\") = 0.4097222222222222\n"
     ]
    },
    {
     "ename": "WordNetError",
     "evalue": "Computing the least common subsumer requires Synset('time.n.01') and Synset('eat.v.01') to have the same part of speech.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWordNetError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [147], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m) = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \n\u001b[0;32m      7\u001b[0m       (similar_sentences[\u001b[38;5;241m0\u001b[39m], similar_sentences[\u001b[38;5;241m1\u001b[39m], symmetric_sentence_similarity_wu(similar_sentences[\u001b[38;5;241m0\u001b[39m], similar_sentences[\u001b[38;5;241m1\u001b[39m])))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m) = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \n\u001b[0;32m      9\u001b[0m       (disparate_sentences[\u001b[38;5;241m0\u001b[39m], disparate_sentences[\u001b[38;5;241m1\u001b[39m], symmetric_sentence_similarity_wu(disparate_sentences[\u001b[38;5;241m0\u001b[39m], disparate_sentences[\u001b[38;5;241m1\u001b[39m])))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m) = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \n\u001b[1;32m---> 12\u001b[0m       (similar_sentences[\u001b[38;5;241m0\u001b[39m], similar_sentences[\u001b[38;5;241m1\u001b[39m], \u001b[43msymmetric_sentence_similarity_resnik\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimilar_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimilar_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimilarity(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m) = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \n\u001b[0;32m     14\u001b[0m       (disparate_sentences[\u001b[38;5;241m0\u001b[39m], disparate_sentences[\u001b[38;5;241m1\u001b[39m], symmetric_sentence_similarity_resnik(disparate_sentences[\u001b[38;5;241m0\u001b[39m], disparate_sentences[\u001b[38;5;241m1\u001b[39m])))\n",
      "Cell \u001b[1;32mIn [142], line 38\u001b[0m, in \u001b[0;36msymmetric_sentence_similarity_resnik\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymmetric_sentence_similarity_resnik\u001b[39m(sentence1, sentence2):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124;03m\"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43msentence_similarity_resnik\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m sentence_similarity_resnik(sentence2, sentence1)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[1;32mIn [142], line 25\u001b[0m, in \u001b[0;36msentence_similarity_resnik\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# For each word in the first sentence\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m synsets1:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Get the similarity value of the most similar word in the other sentence\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([synset\u001b[38;5;241m.\u001b[39mres_similarity(ss,brown_ic) \u001b[38;5;28;01mfor\u001b[39;00m ss \u001b[38;5;129;01min\u001b[39;00m synsets2])\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Check that the similarity could have been computed\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn [142], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# For each word in the first sentence\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m synset \u001b[38;5;129;01min\u001b[39;00m synsets1:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Get the similarity value of the most similar word in the other sentence\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[43msynset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mres_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mss\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbrown_ic\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ss \u001b[38;5;129;01min\u001b[39;00m synsets2])\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Check that the similarity could have been computed\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m best_score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1014\u001b[0m, in \u001b[0;36mSynset.res_similarity\u001b[1;34m(self, other, ic, verbose)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mres_similarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, ic, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    999\u001b[0m \u001b[38;5;124;03m    Resnik Similarity:\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;124;03m    Return a score denoting how similar two word senses are, based on the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;124;03m        have a score of 0 (e.g. N['dog'][0] and N['table'][0]).\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m     ic1, ic2, lcs_ic \u001b[38;5;241m=\u001b[39m \u001b[43m_lcs_ic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lcs_ic\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2358\u001b[0m, in \u001b[0;36m_lcs_ic\u001b[1;34m(synset1, synset2, ic, verbose)\u001b[0m\n\u001b[0;32m   2341\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2342\u001b[0m \u001b[38;5;124;03mGet the information content of the least common subsumer that has\u001b[39;00m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;124;03mthe highest information content value.  If two nodes have no\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2355\u001b[0m \u001b[38;5;124;03minformative subsumer\u001b[39;00m\n\u001b[0;32m   2356\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synset1\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m!=\u001b[39m synset2\u001b[38;5;241m.\u001b[39m_pos:\n\u001b[1;32m-> 2358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WordNetError(\n\u001b[0;32m   2359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing the least common subsumer requires \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to have the same part of speech.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (synset1, synset2)\n\u001b[0;32m   2361\u001b[0m     )\n\u001b[0;32m   2363\u001b[0m ic1 \u001b[38;5;241m=\u001b[39m information_content(synset1, ic)\n\u001b[0;32m   2364\u001b[0m ic2 \u001b[38;5;241m=\u001b[39m information_content(synset2, ic)\n",
      "\u001b[1;31mWordNetError\u001b[0m: Computing the least common subsumer requires Synset('time.n.01') and Synset('eat.v.01') to have the same part of speech."
     ]
    }
   ],
   "source": [
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (similar_sentences[0], similar_sentences[1], symmetric_sentence_similarity(similar_sentences[0], similar_sentences[1])))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (disparate_sentences[0], disparate_sentences[1], symmetric_sentence_similarity(disparate_sentences[0], disparate_sentences[1])))\n",
    "\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (similar_sentences[0], similar_sentences[1], symmetric_sentence_similarity_wu(similar_sentences[0], similar_sentences[1])))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (disparate_sentences[0], disparate_sentences[1], symmetric_sentence_similarity_wu(disparate_sentences[0], disparate_sentences[1])))\n",
    "\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (similar_sentences[0], similar_sentences[1], symmetric_sentence_similarity_resnik(similar_sentences[0], similar_sentences[1])))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (disparate_sentences[0], disparate_sentences[1], symmetric_sentence_similarity_resnik(disparate_sentences[0], disparate_sentences[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fccd6-6987-44b0-93c7-d8a2341810a2",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "Repeat the above reasoning when using the word2vec based similarity assuming that the vector associated to the whole sentence corresponds to the average of the word2vec outputted vector associated to each token of the sentence (provided that the word2vec mapping exists), and then using the cosine similarity to compute the sentence-to-sentence similarity score. (Use Gensim library implementation of pre-trained models). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b781ad6a-6630-4f98-828d-0e859aef0513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "# Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d85f43f4-93bd-43ae-a215-ae63423ece3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Similarity(\"I eat icecream all the time\", \"All I eat is icecream\") = 0.9345358908176422\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Similarity(\"I love movies very much\", \"How is the weather in your country\") = 0.7864689206083615\n"
     ]
    }
   ],
   "source": [
    "def sentence_similarity_word2vec(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        print()\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([glove_vectors.similarity(w1=synset.lemma_names()[0],w2=ss.lemma_names()[0]) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "\n",
    "def symmetric_sentence_similarity_word2vec(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_similarity_word2vec(sentence1, sentence2) + sentence_similarity_word2vec(sentence2, sentence1)) / 2\n",
    "\n",
    "# glove_vectors.similarity(w1=disparate_sentences[0],w2=disparate_sentences[1])\n",
    "\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (similar_sentences[0], similar_sentences[1], symmetric_sentence_similarity_word2vec(similar_sentences[0], similar_sentences[1])))\n",
    "print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % \n",
    "      (disparate_sentences[0], disparate_sentences[1], symmetric_sentence_similarity_word2vec(disparate_sentences[0], disparate_sentences[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
